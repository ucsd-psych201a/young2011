---
title: "Replication of Study by Young & Saxe. (2011, Cognition)"
author: "Cassie Wang (tiw037@ucsd.edu)"
date: "`r format(Sys.time(), '%B %d, %Y')`"
format:
  html:
    toc: true
    toc_depth: 3
---

### Experimental paradigam:

Link: https://ucsd-psych201a.github.io/young2011/

<!-- Replication reports should all use this template to standardize reporting across projects.  These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->

## Introduction

I chose to replicate study by Young and Saxe (https://doi.org/10.1016/j.cognition.2011.04.005) because it provides a fundamental exploration of how intent influences moral judgments across distinct moral domains, specifically harm (e.g., assault) and purity violations (e.g., incest). This experiment highlights that while intent is a crucial factor in moral judgments of harm, it is less influential in purity violations. This result shows different moral evaluations of actions based on intent, offering insight into human decision-making in ethically complicated situations. It aligns with my interests in understanding decision-making process. The study offers a way to explore whether moral judgments are universal or context-specific, making it an excellent candidate for replication.

In my replication study, I will conduct an online survey where participants will read scenarios involving both harm and purity violations, presented in the second-person perspective like Experiment 1A from the original study. Scenarios will be depicted as either intentionally or accidentally. Participants will rate the moral wrongness of each action on a 7-point scale, from “not at all morally wrong” to “very morally wrong.” Challenges in replicating this study may include recruiting a group of participants that reflects the original demographic across online platforms.

## Design Oveview

2 factors are manipulated in the experiments (intentional vs. accidental) (harm vs purity). 1 measure of moral wrongness are taken and repeated after each scenario is prestented. It used between participants design. If changed to within participants design, participants might be able to realize the purpose of the research, hence influence their responses to the study. The experiment will be double blind to reduce demand characteristics. Some of the confounds variables could be educational level or gender.

## Methods

### Power Analysis

Original effect size, power analysis for samples to achieve 80%, 90%, 95% power to detect that effect size. Considerations of feasibility for selecting planned sample size.

### Planned Sample

We plan to recruit participants with an age rang (18-68) through Prolific. The participants have to be fluent in English and live in the United States. We aimed to collect about 200 useable responses. We will verify that participants have not completed a similar task before to avoid familiarity biases.

### Materials

We will use the exact moral judgment scenarios in second-person perspectives used in the original study, including actions representing harm and purity violations with varied intentions (intentional vs. accidental).

### Procedure

Participants will be presented with a single randomly assigned moral judgment scenario and asked to rate its moral wrongness on a 7-point scale. This scale ranges from “not at all morally wrong” (1) to “very morally wrong” (7).

### Analysis Plan

A 2 tailed t-test will be performed to see how domain (harm vs. purity) influence on moral wrongness judgments when actions are accidental.

**Clarify key analysis of interest here** You can also pre-specify additional analyses you plan to do.

### Differences from Original Study

First, the original study recruited participants from Amazon Mechanical Turk (MTurk). We might use Prolific to achieve a broader demographics. We will maintain the same recruitment criteria to minimize the influence that can be caused by differences in demographics and believe that this minor differences will not significantly impact our ability to replicate the original findings.

### Methods Addendum (Post Data Collection)

You can comment this section out prior to final report with data collection.

#### Actual Sample

We collect 5 participants in our pilot study for the demographics of English-speaking adult living in United States.

#### Differences from pre-data collection methods plan

None

## Results

### Data preparation

Data preparation following the analysis plan.

```{r setup, echo=FALSE, results='asis', fig.show='hold'}
# Load necessary libraries
library(dplyr)
library(stringr)
library(jsonlite)

# Define the directory path
file_path <- "/Users/tw/Documents/GitHub/young/data/PilotB"

# List all CSV files in the directory
files <- list.files(file_path, pattern = "\\.csv$", full.names = TRUE)

# Step 1: Read and Combine Data, Excluding Specific Responses
data_list <- lapply(files, function(file) {
  data <- read.csv(file)
  # Exclude rows where response contains {"TakenSurveyBefore":"Yes"}
  data <- data[!grepl('"TakenSurveyBefore":"Yes"', data$response), ]
  return(data)
})
data <- do.call(rbind, data_list)

# Step 2: Filter out the introductory and irrelevant `stimulus` rows
data <- data %>%
  filter(
    !str_detect(
      stimulus, 
      "Welcome to the experiment|You are being invited to participate in a research study|click any key to continue"
    )  # Exclude consent forms or irrelevant prompts
  ) %>%
  filter(!is.na(stimulus))  # Remove rows with missing `stimulus`

# Step 3: Align Scenarios with Responses
# Filter relevant rows
scenarios <- data %>%
  filter(trial_type == "html-keyboard-response" & !is.na(stimulus)) %>%
  select(rt, stimulus)

responses <- data %>%
  filter(trial_type == "survey-likert" & !is.na(response)) %>%
  select(rt, response)

# Align scenarios and responses by row order
aligned_data <- scenarios %>%
  mutate(response = responses$response[1:n()])  # Assign corresponding responses

# Step 4: Remove unnecessary columns and clean up
cleaned_data <- aligned_data %>%
  select(stimulus, response) %>%  # Keep only `stimulus` and `response`
  filter(!is.na(response), !is.na(stimulus))  # Ensure both fields are non-missing

# Step 5: Parse the responses to extract numeric ratings
cleaned_data$rating <- sapply(cleaned_data$response, function(resp) {
  tryCatch({
    parsed_response <- fromJSON(resp)
    as.numeric(parsed_response$Q0)  # Extract and return the numeric rating
  }, error = function(e) {
    NA  # Handle errors gracefully
  })
})

# Step 6: Output cleaned data for verification
print(head(cleaned_data))  # View the first few rows of the cleaned data

# Step 7: Categorization function for intent and domain
categorize_scenario <- function(text) {
  text <- tolower(text)
  
  # Determine intent
  intention <- ifelse(grepl("no idea|forgets|incorrectly", text), "accidental", "intentional")
  
  # Determine domain
  domain <- ifelse(
    grepl("poison|peanuts", text), "harm",
    ifelse(grepl("biological parent|siblings", text), "incest",
           ifelse(grepl("dog meat|urine", text), "ingestion", NA))
  )
  return(c(domain, intention))
}

# Apply categorization to the cleaned data
categories <- t(apply(cleaned_data, 1, function(row) categorize_scenario(row["stimulus"])))
cleaned_data$domain <- categories[, 1]
cleaned_data$intention <- categories[, 2]

# Step 8: Filter data with valid categories
analyzable_data <- cleaned_data %>%
  filter(!is.na(domain), !is.na(intention), !is.na(rating))  # Ensure valid domain, intention, and rating

# Check sufficient levels for ANOVA
if (length(unique(analyzable_data$domain)) < 3 || length(unique(analyzable_data$intention)) < 2) {
  stop("Not enough levels in domain or intention for ANOVA.")
}

# Step 9: Perform 2×3 ANOVA
anova_data <- analyzable_data %>%
  mutate(domain = factor(domain, levels = c("harm", "incest", "ingestion")),
         intention = factor(intention, levels = c("accidental", "intentional")))

anova_result <- aov(rating ~ domain * intention, data = anova_data)
anova_summary <- summary(anova_result)
print(anova_summary)

# Step 10: Calculate mean ratings and standard errors for visualization
mean_ratings <- analyzable_data %>%
  group_by(domain, intention) %>%
  summarise(
    mean_rating = mean(rating, na.rm = TRUE),
    se_rating = sd(rating, na.rm = TRUE) / sqrt(n()),
    .groups = "drop"  # Ungroup after summarization
  )

# Step 11: Plot the results
library(ggplot2)

ggplot(mean_ratings, aes(x = domain, y = mean_rating, fill = intention)) +
  geom_bar(stat = "identity", position = position_dodge(), color = "black", width = 0.6) +
  geom_errorbar(aes(ymin = mean_rating - se_rating, ymax = mean_rating + se_rating), 
                position = position_dodge(0.6), width = 0.2) +
  labs(
    title = "Mean Wrongness Ratings by Domain and Intention",
    x = "Domain",
    y = "Mean Wrongness Rating (with SE)"
  ) +
  theme_minimal() +
  scale_fill_manual(values = c("accidental" = "lightblue", "intentional" = "orange"))
```

### Confirmatory analysis

We will conduct a 2 tailed t-test on domain (harm vs. purity) on moral wrongness ratings. We expect the results to match the result of the original study that illustrated an interaction between intent and domain, indicating that intent plays a more significant role in harm than purity judgments.

*Side-by-side graph with original graph is ideal here*

### Exploratory analyses

Any follow-up analyses desired (not required).

## Discussion

### Summary of Replication Attempt

Open the discussion section with a paragraph summarizing the primary result from the confirmatory analysis and the assessment of whether it replicated, partially replicated, or failed to replicate the original result.

### Commentary

Add open-ended commentary (if any) reflecting (a) insights from follow-up exploratory analysis, (b) assessment of the meaning of the replication (or not) - e.g., for a failure to replicate, are the differences between original and present study ones that definitely, plausibly, or are unlikely to have been moderators of the result, and (c) discussion of any objections or challenges raised by the current and original authors about the replication attempt. None of these need to be long.
